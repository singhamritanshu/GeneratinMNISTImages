{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generating MNIST images .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN4KLynX0AFZ095HzqIj4jt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singhamritanshu/GeneratinMNISTImages/blob/main/Generating_MNIST_images_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCKaTiULPtog"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Krc12eSqQXJi"
      },
      "source": [
        "data = input_data.read_data_sets(\"data/mnist\",one_hot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvt4aeHOQaLX"
      },
      "source": [
        "plt.imshow(data.train.images[13].reshape(28,28),cmap=\"gray\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIV3q2olQgR3"
      },
      "source": [
        "**Defining Generator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vRZIEc1QdL-"
      },
      "source": [
        "def generator(z,reuse=None):\n",
        "    \n",
        "    with tf.variable_scope('generator',reuse=reuse):\n",
        "        \n",
        "        hidden1 = tf.layers.dense(inputs=z,units=128,activation=tf.nn.leaky_relu)\n",
        "        hidden2 = tf.layers.dense(inputs=hidden1,units=128,activation=tf.nn.leaky_relu)\n",
        "        output = tf.layers.dense(inputs=hidden2,units=784,activation=tf.nn.tanh)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvf87N8wQnBp"
      },
      "source": [
        "**Defining Discriminator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVEnSLxVQkwg"
      },
      "source": [
        "def discriminator(X,reuse=None):\n",
        "    \n",
        "    with tf.variable_scope('discriminator',reuse=reuse):\n",
        "        \n",
        "        hidden1 = tf.layers.dense(inputs=X,units=128,activation=tf.nn.leaky_relu)\n",
        "        hidden2 = tf.layers.dense(inputs=hidden1,units=128,activation=tf.nn.leaky_relu)\n",
        "        logits = tf.layers.dense(inputs=hidden2,units=1)\n",
        "        output = tf.sigmoid(logits)\n",
        "        \n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBeVJNJ3QuAg"
      },
      "source": [
        "x = tf.placeholder(tf.float32,shape=[None,784])\n",
        "z = tf.placeholder(tf.float32,shape=[None,100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE8_TbYLQwTy"
      },
      "source": [
        "fake_x = generator(z)\n",
        "D_logits_real = discriminator(x)\n",
        "D_logits_fake = discriminator(fake_x,reuse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_3_3K6EQy3O"
      },
      "source": [
        "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logits_real, labels=tf.ones_like(D_logits_real)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70ofyNB6Q7XB"
      },
      "source": [
        "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logits_fake, labels=tf.zeros_like(D_logits_fake)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAG7s_xNRBk8"
      },
      "source": [
        "D_loss = D_loss_real + D_loss_fake"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bM87XXxiRCaH"
      },
      "source": [
        "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logits_fake,labels=tf.ones_like(D_logits_fake)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dAGhJ0JRKmi"
      },
      "source": [
        "training_vars = tf.trainable_variables()\n",
        "\n",
        "theta_D = [var for var in training_vars if 'dis' in var.name]\n",
        "theta_G = [var for var in training_vars if 'gen' in var.name]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyHyWjvFRLTY"
      },
      "source": [
        "D_optimizer = tf.train.AdamOptimizer(0.001).minimize(D_loss,var_list = theta_D)\n",
        "G_optimizer = tf.train.AdamOptimizer(0.001).minimize(G_loss, var_list = theta_G)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzsB0AQ8RNzm"
      },
      "source": [
        "batch_size = 100\n",
        "num_epochs = 1000\n",
        "\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IattQAHYRQTZ"
      },
      "source": [
        "with tf.Session() as session:\n",
        "    \n",
        "    \n",
        "    #initialize all variables\n",
        "    session.run(init)\n",
        "    \n",
        "    #for each epoch\n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        #select number of batches\n",
        "        num_batches = data.train.num_examples // batch_size\n",
        "        \n",
        "        #for each batch\n",
        "        for i in range(num_batches):\n",
        "            \n",
        "            #get the batch of data according to the batch size\n",
        "            batch = data.train.next_batch(batch_size)\n",
        "            \n",
        "            #reshape the data  \n",
        "            batch_images = batch[0].reshape((batch_size,784))\n",
        "            batch_images = batch_images * 2 - 1\n",
        "            \n",
        "            #sample batch noise\n",
        "            batch_noise = np.random.uniform(-1,1,size=(batch_size,100))\n",
        "            \n",
        "            #define the feed dictionaries with input x as batch_images and noise z as batch noise\n",
        "            feed_dict = {x: batch_images, z : batch_noise}\n",
        "\n",
        "            \n",
        "            #train discriminator and generator\n",
        "            _ = session.run(D_optimizer,feed_dict = feed_dict)\n",
        "            _ = session.run(G_optimizer,feed_dict = feed_dict)\n",
        "\n",
        "            \n",
        "            #compute loss of discriminator and generator\n",
        "            discriminator_loss = D_loss.eval(feed_dict)\n",
        "            generator_loss = G_loss.eval(feed_dict)\n",
        "                      \n",
        "            \n",
        "        #feed the noise to a generator on every 100th epoch and generate an image\n",
        "        if epoch%100==0:\n",
        "            print(\"Epoch: {}, iteration: {}, Discriminator Loss:{}, Generator Loss: {}\".format(epoch,i,discriminator_loss,generator_loss))\n",
        "            \n",
        "            #generate a fake image\n",
        "            _fake_x = fake_x.eval(feed_dict)\n",
        "\n",
        "            #plot the fake image generated by the generator\n",
        "            plt.imshow(_fake_x[0].reshape(28,28))\n",
        "            plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}